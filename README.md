# Summary
`databricks-dolly-15k` is an open source dataset of instruction-following records generated by thousnads of Databricks empoyees in several
of the behavioral categories outlined in the InstructGPT paper, including brainstorming, classification, closed QA, generation, information 
extraction, open QA, and summarization.

[LEGAL]
This dataset is licensed for commercial use ...

# Dataset Overview
Databricks employees were invited to create prompt / response pairs using Google Forms in each of eight different instruction categories, including the seven 
outlined above as well as an open-ended free-form category.  Employees were instructed to avoid using information from any source on the web with 
the exception of Wikipedia, and explicitly instructed to avoid using generative AI in formulating instructions or responses.  Examples of each behavior were 
provided to motivate the types of questions and instructions appropriate to each category.

Halfway through the data generation process, participants were given the option of answering questions posed by other employees. They were 
asked to rephrase the original question and only select questions they could be reasonably expected to answer correctly.  

For questions in the XYZ categories employees were asked to provide a reference passage copied from Wikipedia.  This text may contain bracketed 
citations which we recommend users remove for downstream applications.

# Intended Uses
While immediately valuable for instruction fine tuning large language models, as a corpus of human-generated instruction prompts, 
this dataset also presents a valuable opportunity for synthetic data generation in the methods outlined in the [Self-Instruct](https://arxiv.org/abs/2212.10560) paper. 
For example, employee-generated prompts could be submitted as few-shot examples to a large open language model to generate a corpus of millions of examples of 
instructions in each of the respective InstructGPT categories.  

Likewise, both the instructions and responses present fertile ground for data augmentation.  A paraphrasing model might be used to restate each prompt or short responses, with the 
resulting text associated to the respective ground-truth sample.  Such an approach might provide a form of regularization on the dataset that could allow for 
more robust instruction-following behavior in models derived from these synthetic datasets.

# Dataset Limitations
[LEGAL]
